---
title: 京东商品数据分布式抓取
date: 2018-04-19 14:10:31
tags: 爬虫
---

采用 Scrapy 的 CrawlSpider 来实现爬虫。[GitHub 地址](https://github.com/kuz0/JDSpider)

## 什么是 CrawlSpider ?

`class scrapy.spiders.CrawlSpider`

- 用于爬取有一定规律的网站
- 定义了规则(rules)，提供方便跟进 link 的机制
- 适用于大多数情况，可以根据需要修改部分方法，当然也可以实现自己的 spider

除了从 Spider 继承过来的（必须提供的）属性外，还提供了一个新的属性：

- rules

  是`Rule`对象的集合。每个`Rule`对爬取网站定义了特定的行为。如果多个 rules 匹配了同一个链接，则根据它们在本属性中被定义的顺序，第一个会被使用。<!--more-->

`class scrapy.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)`

- link_extractor：是一个 **Link Extractor 对象**，用于定义需要提取的链接 

- callback：从 link_extractor 中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个 response 作为其第一个参数 。

  注意：当编写爬虫规则时，避免使用 parse 作为回调函数。由于 CrawlSpider 使用 parse 方法来实现其逻辑，如果覆盖了 parse 方法，crawl spider 将会运行失败

- follow：是一个**布尔值(boolean)**，指定了根据该规则从 response 提取的链接是否需要跟进。 如果 callback 为 None，follow 默认设置为 True ，否则默认为 False 

- process_links：指定该 spider 中哪个的函数将会被调用，从 link_extractor 中获取到链接列表时将会调用该函数。该方法主要**用来过滤** 

- process_request：指定该 spider 中哪个的函数将会被调用， 该规则提取到每个 request 时都会调用该函数。 (用来**过滤 request**)

`scrapy.linkextractors.LinkExtractor` 

主要参数：

- allow：满足括号中**正则表达式**的值会被提取，如果为空，则全部匹配 
- deny：与这个**正则表达式**(或正则表达式列表)匹配的 URL 一定不提取 
- allow_domains：**会被提取**的链接的 domains 
- deny_domains：**一定不会被提取**链接的 domains 
- restrict_xpaths：使用 xpath 表达式，和 allow 共同作用**过滤链接**。还有一个类似的 **restrict_css**

## CrawlSpider 如何工作？

1. 由 **start_requests** 对 **start_urls** 中的每一个 url 发起请求，这个请求会被 parse 接收 
2. 在 Spider 里面的 parse 需要我们定义，但 CrawlSpider 定义 parse 去**解析响应**（self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)） 
3. _parse_response 根据有无 **callback**，**follow** 和 **self.follow_links** 执行不同的操作 
4. 其中 **_requests_to_follow** 又会获取 **link_extractor**（这个是传入的LinkExtractor）解析页面得到的**link**（link_extractor.extract_links(response)），对 url 进行加工（process_links，需要自定义），对符合的 link **发起 Request**。使用 process_request(需要自定义）处理响应

## 京东商品数据分布式爬取

京东商品爬取难点在于全站信息，爬取全站所有信息的最好的方法就是使用通用爬虫 CrawlSpider。

### 爬取步骤：

1. 浏览京东网站，寻找商品信息接口 
2. 使用 LinkExtractor 抓取所有链接，用 allow_domains 限制爬取信息接口 
3. 使用正则，抓取商品 id，拼接商品详情链接 
4. 抓取商品详情以及价格 
5. 尝试分布式爬取

### 代码部分：

1. 使用命令，建立 **scrapy 文件**以及 **CrawlSpider 文件**

```shell
scrapy startproject jd
cd jd
scrapy genspider -t crawl jd_spider m.jd.com
```

2. 编写 jd_spider.py 文件，该文件主要负责**爬取链接**以及**商品详情信息**

   [https://github.com/kuz0/JDSpider/blob/master/jd/spiders/jd_spider.py](https://github.com/kuz0/JDSpider/blob/master/jd/spiders/jd_spider.py)


3. 编写 pipeline.py，该文件主要**负责入库** 

   [https://github.com/kuz0/JDSpider/blob/master/jd/pipelines.py](https://github.com/kuz0/JDSpider/blob/master/jd/pipelines.py)


4. 编写 settings.py，该文件主要负责**激活管道**以及**设置下载时间**

   [https://github.com/kuz0/JDSpider/blob/master/jd/settings.py](https://github.com/kuz0/JDSpider/blob/master/jd/settings.py)


5. 添加分布式配置，再另外多台电脑上复制**相同**的一份代码，就可以实现多台同时爬取 

```python
SCHEDULER = "scrapy_redis.scheduler.Scheduler"   #必有项：更改去重对列

# Ensure all spiders share same duplicates filter through redis.
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"   #必有项：利用Redis去重

REDIS_URL = 'redis://xx.xx.xx.xx:xxxx'   #配置连接
```

